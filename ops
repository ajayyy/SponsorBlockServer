{"version":1,"ops":[{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566501066,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDAzNzc1OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524037758"},"message":"Technically, no new endpoint would be required. if the input has a `hash_prefix` key, query by sha1 prefix, if it has a `videoID` key, by video ID. \n\nStoring hashes as binary blobs instead of strings is likely more efficient too; see for example https://stackoverflow.com/a/24011877","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1566586411,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQyNDgxMg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524424812"},"message":"Smart, I don't know how well this will work with how little videoIDs are in this DB, but it definitely could work.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566587355,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQyOTUwMQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524429501"},"message":"On Fri, Aug 23, 2019 at 11:53:31AM -0700, Ajay Ramachandran wrote:\n\u003eI don't know how well this will work with how little videoIDs are in \n\u003ethis DB\n\nIt doesn't matter when we're only returning a single result. That \ndoesn't compromise the user's privacy, because maybe they have requested \nan ID we don't know about.\n\nI think as a first step, I'd be great to modify the existing database to \nadd a `sha1sum` field. It would be good if its type were a binary blob \n(so we can search on it fast), but I don't know if that's easy to handle \nwith JavaScript. The Easy Way‚Ñ¢ would be to make it a plain old string, \nand use a `LIKE %` query, but that is a relatively expensive operation.\n\nWhat do you think?","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1566588635,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQzNTUwMQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524435501"},"message":"Ah, true. I think a blob could work, since it could be converted from a string to blob (base 64 or something)","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1566588677,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQzNTcwMA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524435700"},"message":"One thing to note about this is is could be quite expensive, since it doesn't just send all sponsors for one video, it uses the weighted random generator.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566589687,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQ0MDMwMg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524440302"},"message":"On Fri, Aug 23, 2019 at 12:31:17PM -0700, Ajay Ramachandran wrote:\n\u003e it uses the weighted random generator.\n\nOh, I didn't notice that! Good catch! That function does look involved, \nto say the least! ;-)\n\nIt's probably not a bad idea if we'd do some preliminary evaluation. \n\nOnce we have the hashes in the database, we can have a look at how many \nbits of the hash return how many results (median). My uninformed guess \nis that even sending just 5*16 bits (i.e. 5 hexadecimal chars) will \nreturn just a single result. \n\nThen, I'd go for the tried-and-true method of \"just going with the flow\" \nand make the webextension send 5*16 bits of the hash. if it turns out \nthat this increases cpu time too much, we can either reevaluate the \ncomplete idea, or increase the number of bits required (and return a 400 \nwhen the client tries to use too few bits). \n\nIf you want, I could prepare an sqlite query for adding a new column to \nthe database and populating it with the sha1sums.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1566590779,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQ0NTE3OQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524445179"},"message":"Go for it! I think putting setting it up to start putting sha1sums is a good start, then we can add the rest after. Is there a reason to use sha1 over sha256?","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566591669,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQ0OTM1OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524449358"},"message":"On Fri, Aug 23, 2019 at 01:06:19PM -0700, Ajay Ramachandran wrote:\n\u003eGo for it! I think putting setting it up to start putting sha1sums is a \n\u003egood start,\n\nto be clear: I'm no javascript developer; i'd have to leave implementing \nthat to you. I can work on the sqlite stuff, though.\n\n\u003e Is there a reason to use sha1 over sha256?\n\nit uses less space. we don't require a secure hash for this at all.  \ntechnically, i think, one could implement something similar with for \nexample CRC32 (you'd lose some precision in truncating).\n\nwe do need to mangle the youtube-ids somehow, since they're sparse to \nbegin with.\nso if the user asked for the prefix {video_id:\"abcd*\"}, and we have one \nvideo id (and very likely only 1) with that prefix, we could conclude \nthat the user wanted video abcdefgh, since not many video ids begin with \nabcd. however, if we hash the ids first, then (by design of a secure \nhasing algorithm) we cannot conclude anything about the input.\n\nI'm having a hard time coming up with a satisfying explanation, so feel \nfree to ask for more details!","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1566591881,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQ1MDM1Mw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524450353"},"message":"That makes sense. So really, the only disadvantage is more data throughput and more calculations server side.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566592133,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDQ1MTU2Nw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524451567"},"message":"On Fri, Aug 23, 2019 at 01:24:41PM -0700, Ajay Ramachandran wrote:\n\u003eThat makes sense. So really, the only disadvantage is more data \n\u003ethroughput and more calculations server side.\n\nexactly. and both of those disadvantages can be controlled (by \nincreasing the number of required prefix-bits (and decreasing the \nprivacy of the user)), if it gets out of hand (which I don't think it \nwill, unless we're adding millions of videoIDs).\n\nI'll have a look at the sql stuff over the weekend!","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1566687788,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNDU4NjgxMw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-524586813"},"message":"As promised, here's a quick and reliable way to add the sha1sum column.  \nIt turns out that sqlite has a sha1 function, but only as an extension.  \nYou'll need a C compiler (e.g. gcc or clang) and sqlite-devel (RPM based \nsystems, sqlite-dev on Debian systems I presume).\n\n1. Download sha1.c\n    wget 'https://www.sqlite.org/src/raw/ext/misc/sha1.c?name=df0a667211baa2c0612d8486acbf6331b9f8633fd4d605c17c7cccd26d59c6bd' -O sha1.c\n2. Install sqlite-devel (Fedora) or libsqlite3-dev (Debian/Ubuntu)\n    sudo dnf install sqlite-devel\n    # OR!\n    sudo apt-get install libsqlite3-dev\n3. compile it using\n    gcc -g -fPIC -shared sha1.c -o sha1.so -std=gnu99\n\nFinally, launch sqlite3 database.db from the directory of your compiled \nsha1.so and issue:\n    SELECT load_extension(\"./sha1.so\");\n    BEGIN TRANSACTION;\n    ALTER TABLE sponsorTimes ADD COLUMN sha1sum BLOB;\n    UPDATE sponsorTimes SET sha1sum = sha1(videoID) WHERE sha1sum ISNULL;\n    COMMIT;\n\nOne can then query for hashes using (note the zeroed out missing digits)\n    SELECT * from sponsorTimes WHERE sha1sum BETWEEN \n\t'89b4800000000000000000000000000000000000' AND \n\t'89b4900000000000000000000000000000000000';\n\nThis translates into simple \u003e= and \u003c operations and is therefore pretty \nefficient.\n\nlet me know if that works for you!","files":null},{"type":3,"author":{"id":"df9b0564cf058fe53c90286ecbb74264181358f7"},"timestamp":1567208403,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNjc3ODYwMw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-526778603"},"message":"I was going to file an issue suggesting exactly this, but I have one issue: I don't think there are likely enough videos submitted to make this sufficiently anonymous. It seems to me, that unless and until the database gets huge, the best thing to do would be to just have each client download the whole database, and maintain it locally. It's currently only ~3MB, and I expect it will stay at a manageable size for the foreseeable future. \n\nTo update (probably done every time a video is loaded), you wouldn't need to download the whole database again, just add an API endpoint for getting all entries since `timestamp`.","files":null},{"type":6,"author":{"id":"df9b0564cf058fe53c90286ecbb74264181358f7"},"timestamp":1567208403,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDI3MTQ2NTExOA=="},"target":"b10a58dbd24f2d23f787d1a1ff66ae2b662a0e56c0f313e2a421ab3754e86631","message":"I was going to file an issue suggesting exactly this, but I have one issue: I don't think there are likely enough videos submitted to make this sufficiently anonymous. It seems to me, that unless and until the database gets huge, the best thing to do would be to just have each client download the whole database, and maintain it locally. It's currently only ~3MB, and I expect it will stay at a manageable size for the foreseeable future. \n\nTo update (probably done every time a video is loaded), you wouldn't need to download the whole database again, just add an API endpoint for getting all entries since `timestamp`. \n\nIt's not 100% a fair comparison, but this is what other ad blocking software does: you don't send every URL you visit to a remote server, you download and sync a list that tells you what to block.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1567260825,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNjgzMzg0MA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-526833840"},"message":"On Fri, Aug 30, 2019 at 04:38:17PM -0700, afontenot wrote:\n\u003e I don't think there are likely enough videos submitted to make this \n\u003e sufficiently anonymous. \n\nthat does not matter for k-anonymity: when only 1 hash matches, the \nprobability that the user requested a video that is not in our database \nis high, so even then we can't infer much of anything.\n\n\u003eTo update (probably done every time a video is loaded), you wouldn't \n\u003eneed to download the whole database again, just add an API endpoint for \n\u003egetting all entries since `timestamp`.\n\npartial database exports probably won't scale when/if the project really \ntakes off, both in terms of transfer and storage size. i'm all for the \nfull sqlite dumps, as they bring resiliency to the project.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1567268402,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNjg0MzYyMg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-526843622"},"message":"Is there an easy way to have a database in the browser? I was thinking of setting up a variant of the server that does what you are describing, and then making it so the extension can pull from that local server instead.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1567271594,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNjg0ODc2OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-526848768"},"message":"i'd suggest opening a new bug for that, but in the meantime:\n\ni don't think you can have the sqlite db directly in the addon. two \noptions come to mind, both i think aren't great for that use case:\n\n1. use the localStorage apis:\nhttps://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API/Local_storage\ni'm not familiar with them, but it looks like they only support a \nkey-value lookup and are meant more for config values.  there probably \nis a relatively low limit on how much space (i.e. table rows) can be \nstored.\nthe get-partial-db api would then need to return e.g. json, and the \nextension would insert the values into the store.\n\n2. use a companion application through nativeMessaging:\nhttps://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Native_messaging\nthis way the calls to the sql db could be outsourced to a out-of-browser \nprogram, with the database somewhere on disk.\nthis app would have to be programmed and maintained (extra work), and \nwould likely be somewhat os-specific. afaik this can't be used on \nandroid, but don't quote me on that.\nalso, installing the companion app must be done by the user manually, \nand requires dicking around in ~/.mozilla.  it can't be done via the \nextension for (obvious) security reasons.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1567274868,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyNjg1MzYxNw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-526853617"},"message":"Yea, the other issue is that localStorage does have a very low max storage usage, so it would not scale well.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1567619385,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyODAxMDY0NA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-528010644"},"message":"I think an easier way would be to use an all lower case search of the video IDs. The seem random enough, and when you use 3 characters, you usually get a few results.","files":null},{"type":6,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1567619385,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDI3MzQyODMwNA=="},"target":"61aebffdeedb22c5617bf4a4651fbaeb7d2692ba51cbfbd4beb1c2e419bc0e02","message":"I think an easier way would be to use an all lower case search of the video IDs (not just the first letters, a random subset of letters). The seem random enough, and when you use 3 characters, you usually get a few results.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1567620449,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyODAxODMzMw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-528018333"},"message":"On Wed, Sep 04, 2019 at 10:48:55AM -0700, Ajay Ramachandran wrote:\n\u003eI think an easier way would be to use an all lower case search of the \n\u003evideo IDs. The seem random enough, and when you use 3 characters, you \n\u003eusually get a few results.\n\nhard no!\n\nit's not about getting \"a few results\". this approach gives you nothing, \nbut renders the privacy aspect moot. video ids are sparse, but small and \npartially known. when a user requests a partial id abc?????, the \nprobability that we can associate it with the real id is far higher and \neasier than when the user requests a partial hash \nabcde???????????????????????????????????.\n\nbut it's not just about good practices alone: using (properly \nimplemented) proven-in-the-wild solutions gives users confidence in said \nsolution; one cooked up by a gut-feeling of \"probably good enough\", used \nnowhere else, requires users to trust you that you have carefully \nstudied the trade-offs.\n\nare you worried about performance, implementation work, or something \nelse entirely? I'm sure we can find a solution to it. :-)","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1567620723,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyODAyMDIzMw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-528020233"},"message":"Isn't that the same probability, just that there is more precision with more characters? If someone requests for \"sh3\", their video could be anything that contains \"Sh3\", \"sH3\", etc.\n\nMaybe I am just misunderstanding.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1567621310,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDUyODAyNDA1OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-528024058"},"message":"please just let's all agree to implement the feature as it has been \nproven in the wild (e.g. in the haveibeenpwned-api). diverging from \nknown-working algorithms costs time and gives us nothing at best, \ncompromises users' privacy at worst.\n\nnone of us have studied this field; we can't possibly know where the \npitfalls of a custom approach are.","files":null},{"type":3,"author":{"id":"16a78fe0374a794d34322b7e3ac27bbb03ba2b27"},"timestamp":1575919280,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU2MzM4MzkyNg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-563383926"},"message":"Currently, there are 25k entries in the DB. this means that on average log2(25k) = 15 bit are needed to uniquely identify one video. If we want every request to pull e.g. 16 videos for privacy, then we need to query by a prefix of log2(25k) - log2(16) = 11 bits.\n\nyoutube video ids are base64 strings, so they have 6 bit per character. Hashing the id here doesn't matter regarding privacy I think but even though youtube video ids are pretty obviously random, it's probably still better to hash it to be safe of the distribution. Also it allows for adding other websites later (e.g. hash whole normalized URL which is probably a better idea anyways) and if youtube id algo changes.\n\nI wouldn't use BLOB in sqlite for this since it's harder to filter by, better use hex strings.\n\nHex strings are log2(16)=4 bit per character.\n\nSo right now it would be pretty easy e.g. add a videoID_sha256 column then do queries by the first three characters of the hash (12bits) which would return on average 8 videos.\n\nsqlite3 is able to use indices for GLOB queries (`select where videoID_sha256 GLOB 'a0e*'`), so the performance impact on the server would be minimal.\n\nFor each doubling of the database size, we in theory want to increase the length of the prefix we are looking for by one bit. If we go by hex character prefixes this isn't really possible since we can only increase the prefix every 4 bits or every 16xing of db size.","files":null},{"type":6,"author":{"id":"16a78fe0374a794d34322b7e3ac27bbb03ba2b27"},"timestamp":1575919280,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMyMTk2Mjc5Mw=="},"target":"fd6ac85fc5e2dd0d30843afcf4d04bb4a211d8e5d338b5b30e3af9fadbf6e0d9","message":"Currently, there are 25k entries in the DB. this means that on average log2(25k) = 15 bit are needed to uniquely identify one video. If we want every request to pull e.g. 16 videos for privacy, then we need to query by a prefix of log2(25k) - log2(16) = 11 bits.\n\nyoutube video ids are base64 strings, so they have 6 bit per character. Hashing the id here doesn't matter regarding privacy I think but even though youtube video ids are pretty obviously random, it's probably still better to hash it to be safe of the distribution. Also it allows for adding other websites later (e.g. hash whole normalized URL which is probably a better idea anyways) and if youtube id algo changes.\n\nI wouldn't use BLOB in sqlite for this since it's harder to filter by, better use hex strings.\n\nHex strings are log2(16)=4 bit per character.\n\nSo right now it would be pretty easy e.g. add a videoID_sha256 column then do queries by the first three characters of the hash (12bits) which would return on average 8 videos.\n\nsqlite3 is able to use indices for prefix GLOB queries (`select where videoID_sha256 GLOB 'a0e*'`), so the performance impact on the server would be minimal.\n\nFor each doubling of the database size, we in theory want to increase the length of the prefix we are looking for by one bit. If we go by hex character prefixes this isn't really possible since we can only increase the prefix every 4 bits or every 16xing of db size.","files":null},{"type":3,"author":{"id":"16a78fe0374a794d34322b7e3ac27bbb03ba2b27"},"timestamp":1575919071,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU2MzM4OTU5Mw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-563389593"},"message":"Alternatively to using sqlite3 `GLOB` it would be possible to filter by an exact number of prefix bits by \n\n(a) just storing the sha hash prefix in binary strings (pretty easy since we'll never need more than 32 prefix bit characters which would require 32bytes of storage per entry in this case)\n\n(b) doing range queries on the hex strings: Convert the sha256 to a bigint, set all bits apart from the first or last 11 to zero (=lower bound) and then to 1 (=upper bound), e.g. with `lowerbound = shaint \u0026 ((1\u003c\u003c11) - 1)` and `upperbound = lowerbound + (1 \u003c\u003c 11)`, and then doing the query as\n`select * from \n\n\nProbably neither of these is worth it and just filtering by sha256 prefix is fine, especially since many videos we're looking for aren't even in the result set so just using the first 12 bits is fine until we switch to 16 bit when db size has increased to like 128k entries.","files":null},{"type":6,"author":{"id":"16a78fe0374a794d34322b7e3ac27bbb03ba2b27"},"timestamp":1575919071,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMyMTk2MTAyMw=="},"target":"a3eeab96d3fd8aedd2269498c671cb016973269b44ca36cbfed55122a31516d3","message":"Alternatively to using sqlite3 `GLOB` it would be possible to filter by an exact number of prefix bits by \n\n(a) just storing the sha hash prefix in binary strings (pretty easy since we'll never need more than 32 prefix bit characters which would require 32bytes of storage per entry in this case)\n\n(b) doing range queries on the hex strings: Convert the sha256 to a bigint, set all bits apart from the first or last 11 to zero (=lower bound) and then to 1 (=upper bound), e.g. with `lowerbound = shaint \u0026 ((1\u003c\u003c11) - 1)` and `upperbound = lowerbound + (1 \u003c\u003c 11)`, and then doing the query as\n`select * from sponsorTimes where videoID_sha256 \u003e= lowerbound and videoID_sha256 \u003c upperbound`\n\n\nProbably neither of these is worth it and just filtering by sha256 prefix is fine, especially since many videos we're looking for aren't even in the result set so just using the first 12 bits is fine until we switch to 16 bit when db size has increased to like 128k entries.","files":null},{"type":6,"author":{"id":"16a78fe0374a794d34322b7e3ac27bbb03ba2b27"},"timestamp":1575919196,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMyMTk2MjEyMQ=="},"target":"a3eeab96d3fd8aedd2269498c671cb016973269b44ca36cbfed55122a31516d3","message":"Alternatively to using sqlite3 `GLOB` it would be possible to filter by an exact number of prefix bits by \n\n(a) just storing the sha hash prefix in binary strings (pretty easy since we'll never need more than 32 prefix bit characters which would require 32bytes of storage per entry in this case)\n\n(b) doing range queries on the hex strings: Convert the sha256 to a bigint, set all bits apart from the first or last 11 to zero (=lower bound) and then to 1 (=upper bound), e.g. with `lowerbound = shaint \u0026 ((1\u003c\u003c11) - 1)` and `upperbound = lowerbound + (1 \u003c\u003c 11)`, and then doing the query as\n`select * from sponsorTimes where videoID_sha256 \u003e= lowerbound and videoID_sha256 \u003c upperbound` (abusing alphabetic text ordering here)\n\n\nProbably neither of these is worth it and just filtering by sha256 prefix is fine, especially since many videos we're looking for aren't even in the result set so just using the first 12 bits is fine until we switch to 16 bit when db size has increased to like 128k entries.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1575927093,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU2MzQ0ODk5MA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-563448990"},"message":"I think 16 is too much. Maybe there can be an option to do that, but I think the default should be lower.","files":null},{"type":3,"author":{"id":"1540eb524a93e8c60e3abd2d3bf32b2de505aeec"},"timestamp":1578247418,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU3MDkzMzc0NA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-570933744"},"message":"Note that this wouldn't be able to fix the call to invidio.us:\nhttps://github.com/ajayyy/SponsorBlock/blob/1abc1b9b28d59d2020d26810156cbdfbe3e8f7fa/content.js#L441","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1578247656,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU3MDkzNDA2NA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-570934064"},"message":"@devnoname120 \n\nhttps://github.com/ajayyy/SponsorBlock/issues/189#issuecomment-564265619","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1581391695,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU4NDQ2NDU4NQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-584464585"},"message":"Invidious API usage has been removed.","files":null},{"type":3,"author":{"id":"a823538f8e1f18815645c7d01150dce2cc2bb387"},"timestamp":1590178549,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDYzMjg5NDQwMQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-632894401"},"message":"I came upon SponsorBlock today for the first time and a browser-around let me to this issue. As I personally like to worry about privacy and would not want to send every single video ID to a SponsorBlockServer instance (unless maybe I was operating one within my own network). So I considered looking into this and maybe doing a PR. But first I would like to share some thoughts.\n\nTo start I would like to point at an implementation of k-anonimity for data queries that pre-dates Pwned Passwords: the ‚ÄúSafe Browsing‚Äù (Chrome) and ‚ÄúBlock dangerous and deceptive content‚Äù (Firefox) systems. Information about it is a bit spread around, but some good resources are: [the API Documentation](https://developers.google.com/safe-browsing/v4/) (look for ‚ÄúUpdate API‚Äù), more links collected on the [Mozilla Wiki](https://wiki.mozilla.org/Security/Safe_Browsing), and a [study by Gerbet et al](https://hal.inria.fr/hal-01120186v4/document).\n\nI think this is a good implementation to keep in the back of our heads when continueing the discussion because of the many parallels there are. Safe Browsing is set-up to facilitate a client (web browser) wanting to check a centralised repository (Google‚Äôs collection of mallicious links) for something about a resource (website) without telling the repository about every resource the client uses. Now make the client a video player, the repository SponsorBlockServer, and the resource a YouTube video, and we are in the exact same situation.\n\nSafe Browsing also has a number of points that address some arguments raised by this discussion. You do not need to study it before reading on, just do not be surprised if I reference it again (and again, and again) üòÑ \n\nI will consider the threat vector as: the SponsorBlockServer will get to know every video the user watches. Leave other discussions for later. (If we do not trust the network, we may need obfuscation techniques like [padding](https://www.troyhunt.com/enhancing-pwned-passwords-privacy-with-padding/). If we do not trust the client ‚Ä¶¬†well that is a separate can of worms.)\n\n**Hashing**\n\nI would say hashing is absolutely required. It is the only way to ensure no actual information about the video ID is shared with SponsorBlockServer. Even part of a video ID is still information. Further more, are we sure YouTube video IDs are randomly distributed throughout the possible ID space? Is a `-` just as likely to be part of an ID as an `a`? I do not have a big enough dataset to run this analysis and I do not think we need to do so at all. Hashing means we eliminate that problem.\n\nIt does not really matter what hashing algorithm is chosen. As long as it upholds the default assumptions for a hash we can use it to map both value spaces to eachother: the amount of IDs known to SponsorBlockServer to the amount of IDs that can possibly exist in YouTube.\n\nPwned Password uses SHA-1. But Troy has also explained that he is not really hashing to seriously protect the data. Only to make sure ‚Äúany personal info‚Äù like email addresses ‚Äúin the source data is obfuscated‚Äù. People were also ‚Äúcracking the passwords for fun‚Äù and succeeding at this. (Multiple discussions have happened around this, quotes taken from a [February 2018 blog post](https://www.troyhunt.com/ive-just-launched-pwned-passwords-version-2/).)\n\nI am honestly not sure it matters much for prefixes in our specific case, but Safe Browsing uses SHA-256 and as I said might be closer to our flow. I also do not expect SponsorBlock clients to exist on many platforms that would not have access to SHA-256 primitives.\n\nSo let us say we hash every video identifier with a SHA-256 function. Let us also assume that storage and querying the database is trivial. (Because any debate about `TEXT` vs `BLOB` and optimisation of `WHERE` statements can be had when actual benchmarking can be done.)\n\n**What to hash**\n\nThis is something that was only very shortly touched on in the discussion here by @phiresky:\n\n\u003e [‚Ä¶] it allows for adding other websites later (e.g. hash whole normalized URL which is probably a better idea anyways) and if youtube id algo changes.\n\nThis is a step that once again exists in the Safe Browsing API. They do [URL canonicalisation](https://developers.google.com/safe-browsing/v4/urls-hashing#canonicalization). However my recommendation would be to **not do that**. In my experience anything close to trying to normalise a URL leads to differences between clients. As a quick note: RFC 3986 includes steps for ‚ÄúNormalization‚Äù, the WHATWG URL standard includes steps for ‚Äúequivalence‚Äù, Safe Browsing has steps for ‚ÄúCanonicalization‚Äù. If you are unlucky the language you are writing your client in will have access to anything from 0 to a 100 implementations of any of these.\n\nNot only that, you would also end up having to specify additional steps per platform. YouTube has lots of possible URLs pointing at the same video. Sometimes from entirely different domains (`youtu.be` anyone?).\n\nInstead stick to clear identifiers supplied by the platforms. The YouTube video ID is much less ambiguous than any URL will be. It is also likely that a client already has logic to extract the video ID from a bunch of different sources. Letting them work with that is better.\n\nIf we really value compatibility with other platforms simply generating a URI of our own suffices. Instruct clients to hash `yt:` followed by the YouTube video ID. That would allow simple future expansion without messing with URLs.\n\n**Hash prefix length**\n\nSetting the length of the prefix is a balancing act. The longer the prefix, the less privacy is given. This goes the full range from 0 to 256 bits (in the case of SHA-256).\n\nIf the client asks for a list of possibilities with a 0 bit long prefix it is really just asking for the entire database. This gives complete privacy to the user as the server gets no clues about what video was being accessed. But of course it is a bit annoying for all parties involved as the client would constantly request lots and lots of unneccessary data.\n\nIf the clients asks for a list of possibilities with a 256 bit long prefix it is really just sending the full hash to the server. This gives almost¬π no privacy to the user as the server could match this hash 1:1 with a video ID to know exactly what was being watched. So this is no win for the user.\n\nThe number we are aiming for is somewhere in the middle. I am however not convinced by @phiresky‚Äôs calculation of the prefix length. The length there seems to be argued from the data perspective and not from the user perspective. I do not even think there is any privacy difference depending on how many results the server returns. There is no relation I can see between number of results and the threat vector of not wanting to tell the server about what video I am watching.\n\nFrom the server perspective I do not think we care. There is already an endpoint that allows the client to submit the exact video ID, so obviously we are OK with the 256 bit variant. The database is not secret and infact readily downloadable, so we are also OK with the 0 bit variant.\n\nThe only argument from the server side I can think of is to protect against a number of DoS problems. When the database grows, it may not always be OK to support continuous downloads of the entire thing. Server instances may want to scale up the minimum hash length from 0 to a more managable number in accordance with total database growth. Maybe in such a way that response sizes stay under a certain number of bytes.\n\nFrom the client perspective I think we want to send as little information as possible. This is where the user is and where we care about the privacy. But here are also a number of platform limitations that need to be considered. Like @ajayyy mentioned before about web browsers, some platforms may put limitations on storage. Other platforms may be on limited bandwidth.\n\nSo for a first implementation I was thinking: why not leave it completely up to the client what prefix length it sends and only pick a bottom limit for the server?\n\nWe have 133417 sponsorTimes in the latest DB (just downloaded from the mirror). If we aim to never have an API response include more than a 1000 we need a minimum length that cuts our hash space in approximately 134 blocks. That is just the first 8 bits of the hash (or the first 2 hexadecimal characters).¬≤\n\nIf the client feels this is problematic because of storage, memory, or bandwidth concerns, it can opt to send a longer prefix whilst sacrificing some of its user‚Äôs privacy.\n\n**Future of prefixes**\n\nThe Safe Browsing API once again has an interesting system. The client can download a list of prefixes straight from the server. This is interesting for a number of reasons:\n\n1. If the client encounters a resource where the hash prefix is not in the list at all, it does not need to make any requests. 100% privacy!\n2. The server can make clear to the client exactly what prefixes it will respond to. If a certain 2 bit prefix only includes 10 videos, it does not need to split this up further. If another 8 bit prefix includes 10000 videos maybe it helps to only have 10 bit prefixes defined there.\n\nNow this of course assumes the client has a way to store this list of prefixes and implements logic to keep it up to date. Therefor I am not entirely sure it makes sense to make this a must from day one.\n\n**Pull request**\n\nLike I said at the start, I am considering looking into this and getting started on a pull request. In the mean time I would love to hear everyone‚Äôs input on my thoughts. Whether you think I am completely missing the mark, right on it, or anywhere in between. (Shall we rate it 0 to 256 bits? üòâ)\n\nYou will also find me in the SponsorBlock Discord as Zegnat if you would like to discuss any of the above with me outside of this issue.\n\n---\n\n¬π: almost because if the video ID was not already known to the list, the hash needs to be reversed\n¬≤: this is just math. In reality we may have some uneven distributions through sheer randomness of the data. As our full dataset is still relatively small it would be simple to run the numbers and pick a good cut-off point.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1590180950,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDYzMjkwNTc0Mw==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-632905743"},"message":"you're echoing a lot of my gut feelings about this topic, and clearly \nlaid out. Thanks! I've replied to a few things inline.\n\ntl/dr: full ack! would love to see you open that PR.\n\nOn Fri, May 22, 2020 at 01:16:02PM -0700, Martijn van der Ven wrote:\n\u003eTo start I would like to point at an implementation of k-anonimity for \n\u003edata queries that pre-dates Pwned Passwords: the ‚ÄúSafe Browsing‚Äù \n\u003e(Chrome) and ‚ÄúBlock dangerous and deceptive content‚Äù (Firefox) systems.  \n\ntrue! i can't believe i didn't think of it.\n\n\u003eI would say hashing is absolutely required. It is the only way to \n\u003eensure no actual information about the video ID is shared with \n\u003eSponsorBlockServer. Even part of a video ID is still information.  \n\n+1 to requiring hashing.\n\n\u003eSo let us say we hash every video identifier with a SHA-256 function.  \n\u003eLet us also assume that storage and querying the database is trivial.  \n\u003e(Because any debate about `TEXT` vs `BLOB` and optimisation of `WHERE` \n\u003estatements can be had when actual benchmarking can be done.)\n\n+1 to stop bikeshedding.\n\n\u003e**What to hash**\n[...]\n\u003e\n\u003eInstead stick to clear identifiers supplied by the platforms. The \n\u003eYouTube video ID is much less ambiguous than any URL will be. It is \n[...]\n\u003eIf we really value compatibility with other platforms simply generating \n\u003ea URI of our own suffices. Instruct clients to hash `yt:` followed by \n\u003ethe YouTube video ID. That would allow simple future expansion without \n\nseems like a solid idea.\n\n\u003e**Hash prefix length**\n[...]\n\u003eSo for a first implementation I was thinking: why not leave it \n\u003ecompletely up to the client what prefix length it sends and only pick a \n\u003ebottom limit for the server?\n\nwe should define at least some bounds: retrieving the whole database \nthis way is way more expensive than downloading it, potentially opening \nthe server up to DoS, and personally I'd also let misconfigured clients \n(who send a too specific prefix) hard-fail, so it gets fixed quickly.  \nas a starting point, I'd suggest \u003e=8bits, but \u003c= 50% of the hash.\n\n\u003e**Pull request**\n\u003e\n\u003eLike I said at the start, I am considering looking into this and \n\u003egetting started on a pull request. In the mean time I would love to \n\u003ehear everyone‚Äôs input on my thoughts. Whether you think I am completely \n\u003emissing the mark, right on it, or anywhere in between. (Shall we rate \n\u003eit 0 to 256 bits? üòâ)\n\nyou're getting all 256 from me :)\ni'll definitely review your PR once it's ready (given my limited \njavascript skills it'll be more on the concepts than the implementation \ndetails).","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1590190437,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDYzMjk0MzY3NA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-632943674"},"message":"Sounds great!\n\nI'm going to repeat some things I've already said on discord to have them here more publicly.\n\n\u003e What to hash\n\nI think hashing just the video id is fine for now. I think the service should be added as a separate column (when needed). The service that they are getting info from isn't a large privacy concern and this would more easily allow it to be split to a different database or server in the future depending on growth.\n\n\u003e Prefixing\n\nLike @girst , I think there should be minimums, but allowing it to be changed client-side isn't a bad idea.\n\n\u003e Caching\n\nI want to implement some form of caching in the future but ideally this will hold the actual data instead of just hashes. This would only apply to videos older than some set time period.\n\nThe issue with setting up caching is that fetching the upload time actually takes time. This means that sponsor fetching would be slower for every video as it would fetch the upload time, wait for a response, then fetch times. This is why the current implementation of whitelisting sometimes misses zero second sponsors (though in the latest beta I have added an option to wait before fetching to resolve this issue in exchange for slower sponsor fetching).\n\nI feel that caching can be dealt with separately from this.","files":null},{"type":3,"author":{"id":"7584eef55e79ba5a4542460e11913dc05746a75b"},"timestamp":1593009933,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY0ODg2NTk4NA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-648865984"},"message":"I love that you are tackling privacy issues! I would, however, like to raise one concern regarding the effectiveness of this specific approach. I am not very active/experienced in the field of privacy preserving technology so take this with a grain of salt but I am fairly certain the base idea holds merit.\n\nSome assumptions I'll just make explicit:\n* Attack scenario: the server want's to know which videos a user (as defined by an IP) is watching\n* For simplicity: a user has a fixed/static and unique IP address\n\nObviously hashing doesn't prevent the server from getting the set of videos that were requested. The hash is only a way of specifying a random set of videos, including the one at interest.\nWhat the server will get from an extended video watching session is something like this (assuming k=3):\n\n|Access Time       | 9:08         | 9:21         | 9:35       |\n|-----------|--------------|--------------|------------|\n|**Video 1 id** | \u003cvideo_id\u003e | \u003cvideo_id\u003e | video_id |\n|**Video 2 id** | \u003cvideo_id\u003e | \u003cvideo_id\u003e | video_id |\n|**Video 3 id** | \u003cvideo_id\u003e | \u003cvideo_id\u003e | video_id |\n\nThere are many scenarios in which it is easy to identify which sequence of videos is the correct one. For example, if part 1 and part 2 of a series are in columns 1 \u0026 2 it is very unlikely that those were chosen based on random chance. Similarly if a request for a new video is made after 14 minutes when only one video in the previous column was 14 minutes in length there is an increased likelihood of that video being the one that is watched.\nMore formally this can be modelled as a sequence of conditional probabilities, i.e. what's the probability of `0we7kcmgDOw` following `yNEWkY9D2k4`.  Maximizing that probability sequence would end up with the most likely sequence of videos that a user has watched.\n\nFrom a practical perspective I see that it could be argued that this is still an improvement in privacy, even if an attack like this is sometimes possible (after all it still requires a lot of effort). I would however also like to at least mention the counterpoint that implementing such a scheme could give a false confidence in the privacy afforded by this service.\n\nKeep in mind that human intuition on probability can be very misleading so maybe this attack quickly becomes infeasible. Some actual calculations might be required to assess the feasibility of this approach.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1599235418,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY4NzI0MDU0OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-687240548"},"message":"https://github.com/ajayyy/SponsorBlockServer/pull/127","files":null},{"type":4,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1599235418,"metadata":{"github-id":"MDExOkNsb3NlZEV2ZW50MzczMDQ5MjY5Ng=="},"status":2},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1599235512,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY4NzI0MTM4MA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-687241380"},"message":"Still have to test performance before enabling globally, so it will be just an option for now. I might try enabling it for a certain percentage of all requests (50%) by default and see how it goes.","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1599239885,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY4NzI3ODcxNg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-687278716"},"message":"Thank you @Zegnat, @Joe-Dowd and @ajayyy for your work!\n\nOn Fri, Sep 04, 2020 at 09:05:29AM -0700, Ajay Ramachandran wrote:\n\u003eStill have to test performance before enabling globally, so it will be \n\u003ejust an option for now. I might try enabling it for a certain \n\u003epercentage of all requests (50%) by default and see how it goes.\n\nI guess this statement is regarding the addon, no?\n\nI'd like to use this endpoint with my own client as soon as it's \navailable (right now, it still 404s). Any idea when it's going live, \najayyy?","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1599249911,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY4NzM1NTYyNA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-687355624"},"message":"@girst Yes, it's about the addon. I have noticed that the public database size doubles with this change, so I am going to look into splitting this into a separate database file. It might not be live for a bit.","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1600188975,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY5Mjg0Mzg2OQ==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-692843869"},"message":"@girst The API is now live on the main server","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1600193344,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY5Mjg4NDcxMg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-692884712"},"message":"On Tue, Sep 15, 2020 at 09:56:31AM -0700, Ajay Ramachandran wrote:\n\u003eThe API is now live on the main server\n\nwhat am i doing wrong?\n\n% curl https://sponsor.ajay.app/api/skipSegments/`printf v-ajyq_DeDA|sha256sum|head -c8`\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003ctitle\u003eError\u003c/title\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cpre\u003eCannot GET /api/skipSegments/14e4b9a7\u003c/pre\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\nif i try the test-server(?), i get a different result:\n\n% curl https://sponsor.ajay.app/test/api/skipSegments/`printf v-ajyq_DeDA|sha256sum|head -c8`\nNot Found","files":null},{"type":3,"author":{"id":"3708d7e99c169a5d462136cbf424bcd078c353be"},"timestamp":1600193796,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY5Mjg4ODUzNg==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-692888536"},"message":"Sorry about that, should be good now","files":null},{"type":3,"author":{"id":"51cc80bdfc38f93c3cadc8656d53728cc6e05f64"},"timestamp":1600193924,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY5Mjg4OTY4OA==","github-url":"https://github.com/ajayyy/SponsorBlockServer/issues/25#issuecomment-692889688"},"message":"On Tue, Sep 15, 2020 at 11:16:52AM -0700, Ajay Ramachandran wrote:\n\u003eSorry about that, should be good now\n\nIt is! Thanks again!","files":null}]}